{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "def get_completion(prompt: str, model: str) -> str:\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{prompt} Let's think step by step.\",\n",
    "            },\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"crt_dataset.csv\")\n",
    "questions = df[\"Question\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_responses = {}\n",
    "\n",
    "MODELS = [\"llama3-8b-8192\", \"llama3-70b-8192\", \"mixtral-8x7b-32768\", \"gemma-7b-it\"]\n",
    "\n",
    "for model in MODELS:\n",
    "    model_to_responses[model] = []\n",
    "\n",
    "    for question in questions:\n",
    "        response = get_completion(question, model)\n",
    "        model_to_responses[model].append(response)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classic lateral thinking puzzle!\n",
      "\n",
      "Let's break it down step by step:\n",
      "\n",
      "1. The train derails near the border, with one car in Germany and the other in Poland.\n",
      "2. This means that the train is split across two countries.\n",
      "3. The survivors are still alive, so we need to decide where to bury them.\n",
      "4. Since the train is split across two countries, we need to consider the nationality of the survivors.\n",
      "5. If the survivors are German, they should be buried in Germany.\n",
      "6. If the survivors are Polish, they should be buried in Poland.\n",
      "7. But wait, what if the survivors are not exclusively German or Polish? What if they are, for example, international passengers who don't have a clear nationality?\n",
      "8. In this case, we need to consider the location of the survivors' bodies. Since one car is in Germany and the other in Poland, we can argue that the survivors are \"technically\" in both countries.\n",
      "9. Therefore, we can conclude that the survivors should be buried in both Germany and Poland, with half of the survivors buried in Germany and the other half in Poland.\n",
      "\n",
      "So, the answer is: the survivors should be buried in both Germany and Poland!\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3-8b-8192\"\n",
    "print(model_to_responses[MODEL][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_to_responses)\n",
    "df[\"Question\"] = questions\n",
    "df[[\"Question\"] + MODELS].to_csv(\"cot_with_model_responses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
